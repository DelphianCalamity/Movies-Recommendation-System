{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "input_file = 'ml-latest-small/ratings.csv'\n",
    "headers = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "header_row = None\n",
    "ratings_df = pd.read_csv(input_file, sep=\",\", names=headers, header=header_row, skiprows = 1,\n",
    "                         dtype={'userId': np.int32, 'movieId': np.int32, \n",
    "                                'rating': np.float32,'timestamp': np.int32,\n",
    "                         })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n",
      "9724\n"
     ]
    }
   ],
   "source": [
    "# Making MovieIds, UserIds zero-indexed\n",
    "\n",
    "np_users = ratings_df.userId.values\n",
    "np_items = ratings_df.movieId.values\n",
    "\n",
    "unique_users = np.unique(np_users)\n",
    "unique_items = np.unique(np_items)\n",
    "\n",
    "n_users = unique_users.shape[0]\n",
    "n_items = unique_items.shape[0]\n",
    "\n",
    "print(n_users)\n",
    "print(n_items)\n",
    "\n",
    "max_item = unique_items[-1]\n",
    "\n",
    "# Reconstruct the ratings set's user/movie indices\n",
    "\n",
    "np_users = ratings_df.userId.values\n",
    "np_users[:] -= 1 # Make users zero-indexed\n",
    "# print(np_users)\n",
    "\n",
    "\n",
    "# Mapping unique items down to an array 0..n_items-1\n",
    "z = np.zeros(max_item+1, dtype=int)\n",
    "z[unique_items] = np.arange(n_items)\n",
    "movies_map = z[np_items]\n",
    "\n",
    "np_ratings = ratings_df.rating.values\n",
    "# print(np_ratings.shape[0])\n",
    "ratings = np.zeros((np_ratings.shape[0], 3), dtype=object)\n",
    "ratings[:, 0] = np_users\n",
    "ratings[:, 1] = movies_map\n",
    "ratings[:, 2] = np_ratings\n",
    "\n",
    "# print(ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning dataset to Training and Testing Sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "X_train, X_test = train_test_split(ratings, train_size=0.8)\n",
    "\n",
    "# Ignoring timestamp\n",
    "user_train, movie_train, rating_train = zip(*X_train)\n",
    "train_sparse = coo_matrix((rating_train, (user_train, movie_train)), shape=(n_users, n_items))\n",
    "# print(train_sparse)\n",
    "\n",
    "user_test, movie_test, rating_test = zip(*X_test)\n",
    "test_sparse = coo_matrix((rating_test, (user_test, movie_test)), shape=(n_users, n_items))\n",
    "# print(test_sparse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted alternating least squares (WALS) method \n",
    "\n",
    "Building the Graph - model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/calliope/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.factorization.python.ops import factorization_ops\n",
    "\n",
    "# Default hyperparameters\n",
    "DEFAULT_PARAMS = {\n",
    "    'weights': True,\n",
    "    'latent_factors': 5,\n",
    "    'num_iters': 20,\n",
    "    'regularization': 0.07,\n",
    "    'unobs_weight': 0.01,\n",
    "    'wt_type': 0,\n",
    "    'feature_wt_factor': 130.0,\n",
    "    'feature_wt_exp': 0.08,\n",
    "    'delimiter': '\\t'\n",
    "}\n",
    "\n",
    "# Parameters optimized with hypertuning for the MovieLens data set\n",
    "OPTIMIZED_PARAMS = {\n",
    "    'latent_factors': 34,\n",
    "    'regularization': 9.83,\n",
    "    'unobs_weight': 0.001,\n",
    "    'feature_wt_factor': 189.8,\n",
    "}\n",
    "\n",
    "params = DEFAULT_PARAMS\n",
    "\n",
    "# Create WALS model\n",
    "row_wts = None\n",
    "col_wts = None\n",
    "\n",
    "num_rows = train_sparse.shape[0]\n",
    "num_cols = train_sparse.shape[1]\n",
    "\n",
    "# print(num_rows, num_cols)\n",
    "\n",
    "row_factor = None\n",
    "col_factor = None\n",
    "\n",
    "indices = np.array([[i,j] for i,j in zip(train_sparse.row, train_sparse.col)])\n",
    "values = np.asarray(train_sparse.data, dtype=np.float32)\n",
    "shape = np.asarray([train_sparse.shape[0], train_sparse.shape[1]], dtype=np.int64)\n",
    "# print(indices); print(values); print(shape)\n",
    "\n",
    "input_tensor = tf.SparseTensor(indices=indices, values=values, dense_shape=shape)\n",
    "\n",
    "model = factorization_ops.WALSModel(num_rows, num_cols, \n",
    "                                    n_components=params['latent_factors'], \n",
    "                                    unobserved_weight=params['unobs_weight'],\n",
    "                                    regularization=params['regularization'],\n",
    "                                    row_weights=row_wts, \n",
    "                                    col_weights=col_wts)\n",
    "\n",
    "# Retrieve the row and column factors\n",
    "row_factor = model.row_factors[0]\n",
    "col_factor = model.col_factors[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=input_tensor.graph)\n",
    "\n",
    "row_update_op = model.update_row_factors(sp_input=input_tensor)[1]\n",
    "col_update_op = model.update_col_factors(sp_input=input_tensor)[1]\n",
    "\n",
    "sess.run(model.initialize_op)\n",
    "sess.run(model.worker_init)\n",
    "\n",
    "row_update_prep_gramian_op = model.row_update_prep_gramian_op\n",
    "col_update_prep_gramian_op = model.col_update_prep_gramian_op\n",
    "\n",
    "init_row_update_op = model.initialize_row_update_op\n",
    "init_col_update_op = model.initialize_col_update_op\n",
    "\n",
    "num_iterations = 1\n",
    "\n",
    "for _ in range(num_iterations):    \n",
    "#     sess.run(row_update_prep_gramian_op)\n",
    "#     sess.run(init_row_update_op)\n",
    "#     sess.run(row_update_op)\n",
    "    row_update_prep_gramian_op.run(session=sess)\n",
    "    init_row_update_op.run(session=sess)\n",
    "    row_update_op.run(session=sess)\n",
    "    \n",
    "    col_update_prep_gramian_op.run(session=sess)\n",
    "    init_col_update_op.run(session=sess)\n",
    "    col_update_op.run(session=sess)\n",
    "\n",
    "\n",
    "#     sess.run(col_update_prep_gramian_op)\n",
    "#     sess.run(init_col_update_op)\n",
    "#     sess.run(col_update_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate output factor matrices\n",
    "output_row = row_factor.eval(session=sess)\n",
    "output_col = col_factor.eval(session=sess)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3edb1d4759e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movie'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'row'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'col'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 536\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = os.path.join(\"WALS\", 'model')\n",
    "\n",
    "os.makedirs(model_dir)\n",
    "np.save(os.path.join(model_dir, 'user'), np_users)\n",
    "np.save(os.path.join(model_dir, 'movie'), movies_map)\n",
    "np.save(os.path.join(model_dir, 'row'), row_factor)\n",
    "np.save(os.path.join(model_dir, 'col'), col_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Compute RMSE and MAE between predicted and actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:train RMSE = 3.65\n",
      "INFO:tensorflow:test RMSE = 3.65\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_rmse(output_row, output_col, actual):\n",
    "    mse = 0\n",
    "    for i in range(actual.data.shape[0]):\n",
    "        row_pred = output_row[actual.row[i]]\n",
    "        col_pred = output_col[actual.col[i]]\n",
    "        err = actual.data[i] - np.dot(row_pred, col_pred)\n",
    "        mse += err * err\n",
    "    mse /= actual.data.shape[0]\n",
    "    rmse = math.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "train_rmse = get_rmse(output_row, output_col, train_sparse)\n",
    "test_rmse = get_rmse(output_row, output_col, test_sparse)    \n",
    "    \n",
    "tf.logging.info('train RMSE = %.2f' % train_rmse)\n",
    "tf.logging.info('test RMSE = %.2f' % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int k = 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
