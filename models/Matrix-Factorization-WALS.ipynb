{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import sparse_tensor\n",
    "from tensorflow.python.ops import sparse_ops\n",
    "from tensorflow.contrib.factorization.python.ops import factorization_ops\n",
    "from tensorflow.contrib.factorization.python.ops import factorization_ops_test_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "np_matrix_to_tf_sparse = factorization_ops_test_utils.np_matrix_to_tf_sparse\n",
    "\n",
    "\n",
    "def np_matrix_to_tf_sparse(np_matrix, i,\n",
    "                           row_slices=None,\n",
    "                           col_slices=None,\n",
    "                           transpose=False,\n",
    "                           shuffle=False):\n",
    "  \"\"\"Simple util to slice non-zero np matrix elements as tf.SparseTensor.\"\"\"\n",
    "  indices = np.nonzero(np_matrix)\n",
    "\n",
    "  # Only allow slices of whole rows or whole columns.\n",
    "  assert not (row_slices is not None and col_slices is not None)\n",
    "\n",
    "  if row_slices is not None:\n",
    "    selected_ind = np.concatenate([np.where(indices[0] == (r-i))[0] for r in row_slices], 0)\n",
    "    in_real = (indices[0][selected_ind], indices[1][selected_ind])\n",
    "    indices = (indices[0][selected_ind]+i, indices[1][selected_ind])\n",
    "\n",
    "  # if col_slices is not None:\n",
    "  #   selected_ind = np.concatenate([np.where(indices[1] == c)[0] for c in col_slices], 0)\n",
    "  #   indices = (indices[0][selected_ind], indices[1][selected_ind])\n",
    "  #   in_real = indices\n",
    "\n",
    "  if shuffle:\n",
    "    shuffled_ind = [x for x in range(len(indices[0]))]\n",
    "    random.shuffle(shuffled_ind)\n",
    "    indices = (indices[0][shuffled_ind], indices[1][shuffled_ind])\n",
    "\n",
    "  ind = (np.concatenate((np.expand_dims(indices[1], 1), np.expand_dims(indices[0], 1)), 1).astype(np.int64) if\n",
    "         transpose else np.concatenate((np.expand_dims(indices[0], 1), np.expand_dims(indices[1], 1)), 1).astype(np.int64))\n",
    "\n",
    "  val = np_matrix[in_real].astype(np.float32)\n",
    "  shape = (np.array([max(indices[1]) + 1, max(indices[0]) + 1]).astype(np.int64)\n",
    "           if transpose else np.array(\n",
    "               [max(indices[0]) + 1, max(indices[1]) + 1]).astype(np.int64))\n",
    "  return sparse_tensor.SparseTensor(ind, val, shape)\n",
    "\n",
    "\n",
    "def get_MAE(output_row, output_col, actual):\n",
    "    mae = 0\n",
    "    for i in range(actual.data.shape[0]):\n",
    "        row_pred = output_row[actual.row[i]]\n",
    "        col_pred = output_col[actual.col[i]]\n",
    "        mae += abs(actual.data[i] - np.dot(row_pred, col_pred))\n",
    "    mae /= actual.data.shape[0]\n",
    "    return mae\n",
    "\n",
    "def get_RMSE(output_row, output_col, actual):\n",
    "    mse = 0\n",
    "    for i in range(actual.data.shape[0]):\n",
    "        row_pred = output_row[actual.row[i]]\n",
    "        col_pred = output_col[actual.col[i]]\n",
    "        err = actual.data[i] - np.dot(row_pred, col_pred)\n",
    "        mse += err * err\n",
    "    mse /= actual.data.shape[0]\n",
    "    rmse = math.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def evaluate_model(sess, train_sparse, test_sparse, row_factor, col_factor):\n",
    "\n",
    "    train_rmse = get_RMSE(row_factor, col_factor, train_sparse)\n",
    "    test_rmse = get_RMSE(row_factor, col_factor, test_sparse)    \n",
    "\n",
    "#     print('train RMSE: ', train_rmse)\n",
    "#     print('test RMSE: ', test_rmse)\n",
    "    tf.logging.info('train RMSE = %f' % train_rmse)\n",
    "    tf.logging.info('test RMSE = %f' % test_rmse)\n",
    "\n",
    "    train_mae = get_MAE(row_factor, col_factor, train_sparse)\n",
    "    test_mae = get_MAE(row_factor, col_factor, test_sparse)    \n",
    "#     print('train MAE: ', train_mae)\n",
    "#     print('test MAE: ', test_mae)\n",
    "\n",
    "    tf.logging.info('train MAE = %f' % train_mae)\n",
    "    tf.logging.info('test MAE = %f' % test_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Dataset....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading Dataset....\\n\")\n",
    "\n",
    "\n",
    "input_file = '../datasets/ml-latest/ratings.csv'\n",
    "headers = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "header_row = None\n",
    "ratings_df = pd.read_csv(input_file, sep=\",\", names=headers, header=header_row, skiprows = 1,\n",
    "                         dtype={'userId': np.int32, 'movieId': np.int32, \n",
    "                                'rating': np.float32,'timestamp': np.int32,\n",
    "                         })\n",
    "\n",
    "ratings_df = ratings_df.head(len(ratings_df)//10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preprocessing....\n",
      "\n",
      "28666 28267\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Preprocessing....\\n\")\n",
    "\n",
    "np_users = ratings_df.userId.values\n",
    "np_items = ratings_df.movieId.values\n",
    "\n",
    "unique_users = np.unique(np_users)\n",
    "unique_items = np.unique(np_items)\n",
    "\n",
    "n_users = unique_users.shape[0]\n",
    "n_items = unique_items.shape[0]\n",
    "\n",
    "print(n_users, n_items)\n",
    "\n",
    "max_item = unique_items[-1]\n",
    "\n",
    "# Reconstruct the ratings set's user/movie indices\n",
    "np_users = ratings_df.userId.values\n",
    "np_users[:] -= 1 # Make users zero-indexed\n",
    "\n",
    "# Mapping unique items down to an array 0..n_items-1\n",
    "z = np.zeros(max_item+1, dtype=int)\n",
    "z[unique_items] = np.arange(n_items)\n",
    "movies_map = z[np_items]\n",
    "\n",
    "np_ratings = ratings_df.rating.values\n",
    "# print(np_ratings.shape[0])\n",
    "ratings = np.zeros((np_ratings.shape[0], 3), dtype=object)\n",
    "ratings[:, 0] = np_users\n",
    "ratings[:, 1] = movies_map\n",
    "ratings[:, 2] = np_ratings\n",
    "\n",
    "X_train, X_test = train_test_split(ratings, train_size=0.8)\n",
    "\n",
    "# Ignoring timestamp\n",
    "user_train, movie_train, rating_train = zip(*X_train)\n",
    "train_sparse = coo_matrix((rating_train, (user_train, movie_train)), shape=(n_users, n_items))\n",
    "# print(train_sparse)\n",
    "\n",
    "user_test, movie_test, rating_test = zip(*X_test)\n",
    "test_sparse = coo_matrix((rating_test, (user_test, movie_test)), shape=(n_users, n_items))\n",
    "# print(test_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Model....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding Model....\\n\")\n",
    "\n",
    "# Default hyperparameters\n",
    "DEFAULT_PARAMS = {\n",
    "    'weights': True,\n",
    "    'latent_factors': 6000,\n",
    "    'num_iters': 20,\n",
    "    'regularization': 2.17,\n",
    "    'unobs_weight': 0.01,\n",
    "    'wt_type': 0,\n",
    "    'feature_wt_factor': 130.0,\n",
    "    'feature_wt_exp': 0.08,\n",
    "    'delimiter': '\\t'\n",
    "}\n",
    "\n",
    "# Parameters optimized with hypertuning for the MovieLens data set\n",
    "OPTIMIZED_PARAMS = {\n",
    "    'latent_factors': 34,\n",
    "    'regularization': 9.83,\n",
    "    'unobs_weight': 0.001,\n",
    "    'feature_wt_factor': 189.8,\n",
    "}\n",
    "\n",
    "params = DEFAULT_PARAMS\n",
    "\n",
    "# Create WALS model\n",
    "row_wts = None\n",
    "col_wts = None\n",
    "\n",
    "num_rows = train_sparse.shape[0]\n",
    "num_cols = train_sparse.shape[1]\n",
    "\n",
    "sess = tf.Session()  #graph=input_tensor.graph)\n",
    "\n",
    "model = factorization_ops.WALSModel(num_rows, num_cols, \n",
    "                                    n_components=params['latent_factors'],\n",
    "                                    # num_row_shards=2,\n",
    "                                    # num_col_shards=3,\n",
    "                                    unobserved_weight=params['unobs_weight'],\n",
    "                                    regularization=params['regularization'],\n",
    "                                    row_weights=row_wts, \n",
    "                                    col_weights=col_wts)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(model.initialize_op)\n",
    "sess.run(model.worker_init)\n",
    "\n",
    "sp_feeder_row = array_ops.sparse_placeholder(dtypes.float32)\n",
    "(_, slice_row, unreg_loss_row, reg_row, _) = model.update_row_factors(sp_input=sp_feeder_row, transpose_input=False)\n",
    "factor_loss_row = unreg_loss_row + reg_row\n",
    "\n",
    "\n",
    "sp_feeder_col = array_ops.sparse_placeholder(dtypes.float32)\n",
    "(_, slice_col, unreg_loss_col, reg_col,_) = model.update_col_factors(sp_input=sp_feeder_col, transpose_input=True)\n",
    "factor_loss_col = unreg_loss_col + reg_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Dataset to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparation for Training....\n",
      "\n",
      "28666 111\n",
      "28267 110\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nPreparation for Training....\\n\")\n",
    "\n",
    "# # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# batch = 256\n",
    "# X_train = train_sparse.tocsr()\n",
    "\n",
    "# k_rows = num_rows // batch\n",
    "# k_cols = num_cols // batch\n",
    "\n",
    "# print(num_rows, k_rows)\n",
    "# print(num_cols, k_cols)\n",
    "\n",
    "# # ~~~~~~~~~ Cutting smaller Row Slices - mini batching ~~~~~~~~~ #\n",
    "# input_scattered_rows = []\n",
    "# for i in range(k_rows+1):\n",
    "\n",
    "#     to = (i+1)*batch\n",
    "#     if to > num_rows:\n",
    "#         to = num_rows\n",
    "\n",
    "#     x = X_train[i*batch : (i+1)*batch]\n",
    "#     x = x.toarray()\n",
    "#     slice = np_matrix_to_tf_sparse(x, i*batch, row_slices=np.arange(i*batch, to)).eval(session=sess)\n",
    "#     input_scattered_rows.append(slice)\n",
    "\n",
    "    \n",
    "# # ~~~~~~~~~ Cutting smaller Column Slices - mini batching ~~~~~~~~~ #\n",
    "# input_scattered_cols = []\n",
    "# for i in range(k_cols+1):\n",
    "\n",
    "#     to = (i+1)*batch\n",
    "#     if to > num_cols:\n",
    "#         to = num_cols\n",
    "\n",
    "#     x = X_train[:, i*batch : (i+1)*batch]\n",
    "#     x = np.transpose(x.toarray())\n",
    "\n",
    "#     slice = np_matrix_to_tf_sparse(x, i*batch, row_slices=np.arange(i*batch, to)).eval(session=sess)\n",
    "#     input_scattered_cols.append(slice)\n",
    "\n",
    "# # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training....\n",
      "\n",
      "\n",
      "Evaluating..:  0 / 10\n",
      "INFO:tensorflow:train RMSE = 3.455636\n",
      "INFO:tensorflow:test RMSE = 3.467250\n",
      "INFO:tensorflow:train MAE = 3.289851\n",
      "INFO:tensorflow:test MAE = 3.301126\n",
      "\n",
      "Evaluating..:  4 / 10\n",
      "INFO:tensorflow:train RMSE = 1.982036\n",
      "INFO:tensorflow:test RMSE = 3.244677\n",
      "INFO:tensorflow:train MAE = 1.582439\n",
      "INFO:tensorflow:test MAE = 3.038934\n",
      "\n",
      "Evaluating..:  8 / 10\n",
      "INFO:tensorflow:train RMSE = 1.948663\n",
      "INFO:tensorflow:test RMSE = 3.258027\n",
      "INFO:tensorflow:train MAE = 1.540657\n",
      "INFO:tensorflow:test MAE = 3.054977\n",
      "\n",
      "Evaluating..:  10 / 10\n",
      "INFO:tensorflow:train RMSE = 1.946321\n",
      "INFO:tensorflow:test RMSE = 3.258638\n",
      "INFO:tensorflow:train MAE = 1.537408\n",
      "INFO:tensorflow:test MAE = 3.055770\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining....\\n\")\n",
    "num_iterations = 10\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    #####################################################################################\n",
    "    # ROW SWEEP\n",
    "    model.row_update_prep_gramian_op.run(session=sess)                  # Step 1\n",
    "    model.initialize_row_update_op.run(session=sess)                    # Step 2\n",
    "    for inp in input_scattered_rows:\n",
    "        feed_dict = {sp_feeder_row: inp}\n",
    "        slice_row.run(session=sess, feed_dict=feed_dict)                # Step 3\n",
    "    row_factors = [x.eval(session=sess) for x in model.row_factors]\n",
    "\n",
    "    # COLUMN SWEEP\n",
    "    model.col_update_prep_gramian_op.run(session=sess)                  # Step 1\n",
    "    model.initialize_col_update_op.run(session=sess)                    # Step 2\n",
    "    for inp in input_scattered_cols:\n",
    "        feed_dict = {sp_feeder_col: inp}\n",
    "        slice_col.run(session=sess, feed_dict=feed_dict)                # Step 3\n",
    "    col_factors = [x.eval(session=sess) for x in model.col_factors]\n",
    "    #####################################################################################\n",
    "\n",
    "    if i%4 == 0:\n",
    "        print(\"\\nEvaluating..: \", i, \"/\", num_iterations)\n",
    "        evaluate_model(sess, train_sparse, test_sparse, row_factors[0], col_factors[0])\n",
    "\n",
    "# if num_iterations%3 == 0:\n",
    "print(\"\\nEvaluating..: \", num_iterations, \"/\", num_iterations)\n",
    "evaluate_model(sess, train_sparse, test_sparse, row_factors[0], col_factors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Saving Model....\\n\")\n",
    "\n",
    "# Evaluate output factor matrices\n",
    "output_row = row_factor.eval(session=sess)\n",
    "output_col = col_factor.eval(session=sess)\n",
    "\n",
    "model_dir = os.path.join(\"WALS\", 'model')\n",
    "os.makedirs(model_dir)\n",
    "np.save(os.path.join(model_dir, 'user'), np_users)\n",
    "np.save(os.path.join(model_dir, 'movie'), movies_map)\n",
    "np.save(os.path.join(model_dir, 'row'), output_row)\n",
    "np.save(os.path.join(model_dir, 'col'), output_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Compute RMSE and MAE between predicted and actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(sess, train_sparse, test_sparse, row_factor, col_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int k = 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
